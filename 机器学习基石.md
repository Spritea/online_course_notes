# 机器学习基石
## The Learning Problem    
1. 适用机器学习的条件  
    * 存在范式
    * 普通的程序不方便定义
    * 有关于范式的数据
2. 机器学习模型=算法+假设
3. **机器学习ML**：用数据来计算假设，逼近真实的分布  
**数据挖掘DM**：从大数据中找到**感兴趣**的性质，如果ML的假设就是感兴趣的性质，那么此时ML=DM  
**人工智能AI**：让机器具备智能行为，机器学习是实现人工智能的方式   
**统计Statistics**: 使用数据来对某个未知过程做推理，是从数学出发的，可以用来实现机器学习
## Learning to Answer Yes/No
1. 感知器假设+算法，感知器就是线性分类器的一种  
2. 感知器算法    
   * Perceptron Learning Algorithm=> W(t+1)= W(t)+y[n(t)]x[n(t)]
   * h-正负分割线就是W(t)的法线   
   * 证明了PLA算法会停/收敛，前提：数据线性可分   
3. 处理带噪声的线性
   * 没有闭式解，np-hard问题
   * Pocket：以迭代的方式最小化误差，比PLA慢
   * 通过保持最好的线在手里，来调整PLA
   * 学习算法的类型  
      * 监督式
      * 无监督
      * 半监督
      * 强化学习
4. 学习的可行性分析
   * No Free Launch：针对某一领域所有问题，所有算法的期望性能是相同的
   * Hoeffding不等式：样本足够大时，样本分布接近真实分布，故能用抽样来估计真实的分布
## Training versus Testing
1. 讨论误差上限，对假设进行分类
   * 有效线的数量是有限的
   * 有效假设的数量=有效线的数量
2. 成长函数=假设数量对于样本数量的函数
3. 断点k：growth function对于k的值<2^k(shatter)，关注最小断点
## Theory of Generalization
1. minimum break point对假设数量的限制:固定minimum break point，固定样本数量，便固定了假设的数量，也就是dichotomy（二分）的条数
2. bounding function/上限函数:对于mbp的最大假设数量
