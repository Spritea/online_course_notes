# 机器学习技法Part 2   

<sub>*Combining Predictive Features: Aggregation Models*</sub>   

## Blending and Bagging
1. motivation-better performance
2. uniform blending
   * reduces variance for most stable performance
3. linear blending=Lin model+ hypotheses as transform
4. any blending/stacking, easily overfitted
5. bootstrap aggregation/bagging
   * bagging works very well if base algorithm sensitive to data randomness